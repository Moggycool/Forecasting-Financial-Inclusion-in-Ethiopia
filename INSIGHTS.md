# Insights & Data Limitations

This document summarizes **evidence-based insights** and **data limitations** derived from the exploratory data analysis (EDA) outputs generated by the reproducible pipeline.

All claims are backed by tables located in:

- data/processed/task2_eda_outputs/

---

## Insight 1 — Multi-Typed Dataset Enables Hybrid Modeling

**Claim:** The dataset supports a **hybrid modeling approach** rather than a single-type panel.  
It combines observations, events, impact links, and targets within one unified structure.

**Evidence:**

- `counts__record_type.csv` → multiple `record_type` categories  
- `events.csv` → event records present  
- `impact_links.csv` → causal relationship records  
- `temporal_range__targets.csv` → target records (if present)

**Interpretation:** The structure naturally supports a **multi-layer modeling strategy**:

| Layer | Purpose |
|----------|------------------------------|
| Observation | Indicator trajectories over time |
| Event | Qualitative shocks/interventions |
| Impact Link | Hypothesized causal relationships |
| Target | Forecast goals/benchmarks |

This enables:

- event-aware forecasting  
- causal feature engineering  
- policy impact analysis  
- goal-oriented evaluation  

**Confidence:** **High** — directly verified by record counts and exported tables.

---

## Insight 2 — Temporal Coverage Differs by Record Type

**Claim:** Observation, event, and target records may cover **different time spans**, limiting backtesting and causal attribution.

**Evidence:**

- `temporal_range.csv`
- `temporal_range__observations.csv`
- `temporal_range__events.csv`
- `temporal_range__targets.csv`

**Interpretation:** If coverage is misaligned:

- pre/post comparisons become unreliable  
- event studies lose credibility  
- consistent forecast horizons are impossible  
- causal attribution weakens  

Temporal alignment is therefore critical before modeling.

**Confidence:** **High** — based on computed min/max dates (severity depends on specific outputs).

---

## Insight 3 — Indicator Coverage is Uneven

**Claim:** Some indicators have **too few time points** to support robust time-series forecasting.

**Evidence:**

- `indicator_coverage.csv` → observation counts and date spans  
- Optional derived list → `sparse_indicators_nobs_le2.csv`

**Interpretation:** Indicators with 1–2 observations should be:

- treated as cross-sectional only  
- excluded from forecasting  
- supplemented with proxy series  
- pooled via hierarchical/Bayesian approaches  

Sparse time series inflate uncertainty and risk misleading trends.

**Confidence:** **High** for uneven coverage; **Medium** for exact threshold choice.

---

## Insight 4 — Taxonomy Coverage is Imbalanced

**Claim:** Indicators are not evenly distributed across taxonomy groups (`pillar`, `category`), which can bias conclusions.

**Evidence:**

- `counts__pillar.csv`
- `counts__category.csv`

**Interpretation:** If one pillar dominates:

- summary statistics over-represent that pillar  
- models perform best only where coverage is highest  
- aggregate narratives may hide weaknesses elsewhere  

Balanced analysis requires **pillar-aware evaluation**.

**Confidence:** **Medium–High** — depends on completeness of taxonomy labeling.

---

## Insight 5 — Relationship Integrity is Critical

**Claim:** Impact links referencing missing or unresolved parent events represent a **major data quality risk**.

**Evidence:**

### Diagnostics (if issues exist)

- `invalid_impact_links_missing_parent.csv`
- `invalid_impact_links_unresolved_parent.csv`

### EDA Tables

- `impact_links.csv`
- `events.csv`

**Interpretation:** Broken relationships mean:

- event → indicator mappings fail  
- causal modeling becomes unreliable  
- qualitative claims are not auditable  

The enrichment pipeline’s diagnostics make these problems **detectable and fixable**.

**Confidence:** **High** — pipeline-generated checks.

---

## Data Limitations (Rubric-Aligned)

These limitations should be explicitly acknowledged in reporting and modeling decisions.

---

### Limitation 1 — Sparse & Irregular Longitudinal Coverage

**What it is:** Many indicators are not measured regularly or have very few observations.

**Where it appears:**

- `indicator_coverage.csv`

**Why it matters:** Time-series models require sufficient history. Sparse data increases variance and weakens forecasts.

**Mitigation:**

- restrict forecasting to well-covered indicators  
- use proxies  
- pool related indicators  
- label sparse series as descriptive only  

---

### Limitation 2 — Mixed Definitions & Data-Generating Processes

**What it is:** Indicators mix:

- survey measures  
- administrative/platform metrics  

**Where it appears:**

- indicator metadata  
- mixed types in `indicator_coverage.csv`

**Why it matters:** Survey ownership ≠ administrative registrations.  
Direct comparisons can create misleading conclusions.

**Mitigation:**

- document definitions per `indicator_code`  
- avoid cross-type level comparisons  
- prefer within-indicator trends  

---

### Limitation 3 — Aggregation Risks

**What it is:** Mixing national totals with subgroup indicators (urban/rural, gender, region).

**Where it appears:**

- subgroup fields (if present)  
- uneven pillar/category counts

**Why it matters:** Models may learn spurious relationships across aggregation levels.

**Mitigation:**

- stratify analysis  
- compare only like-for-like series  
- validate aggregation consistency  

---

### Limitation 4 — Event-to-Measurement Time Lag

**What it is:** Policy or product changes may impact indicators after a delay (or anticipation effects).

**Where it appears:**

- `events.csv`
- `impact_links.csv`
- observation timing tables

**Why it matters:** Naive “event then change” logic can misattribute effects.

**Mitigation:**

- incorporate lag windows  
- use pre/post evaluation periods  
- avoid single-date comparisons  

---

### Limitation 5 — Heterogeneous Confidence Across Indicators

**What it is:** Data quality varies widely between indicators and time periods.

**Where it appears:**

- uneven coverage  
- diagnostic anomalies  

**Why it matters:** A single global confidence statement is misleading.

**Mitigation:**

- report per-indicator confidence  
- use uncertainty intervals  
- create data quality tiers  

---

### Code Validations

**Codebook validation:** All categorical code fields (record_type, category, pillar, value_type, etc.) are validated against `reference_codes.xlsx`. Any violations are exported to
`reference_codes__unknown_codes.csv` and `reference_codes__invalid_applies_to.csv`.

- Validation against the NBE master codebook returned 0 unknown codes , 0 applies_to violations , and 0 reference duplicates across 43 rows . The audit summary ( reference_codes__summary.csv) lists validated fields as record_type,pillar,category,value_type,source_type,confidence,gender,location. The coverage artifact ( reference_codes__applies_to_coverage.csv) further demonstrates execution of record-type context checks (eg, pillar: 33 checked, 0 invalid; category: 10 checked, 0 invalid ), while record_typeis validated for allowed codes but correctly excluded from context checking (as it defines the context).

## Summary

The unified dataset provides a **rich foundation for hybrid, event-aware forecasting and policy analysis**, but:

- coverage is uneven  
- definitions differ  
- temporal alignment matters  
- relationship integrity must be validated  

Careful preprocessing, stratification, and uncertainty-aware modeling are required to produce reliable conclusions.

---

**Tip:** Re-run the pipeline anytime new enrichment or raw data is added:

```text
python scripts/apply_enrichment.py ...
python scripts/run_exploration.py ...
```
