{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "3079cc71",
            "metadata": {},
            "source": [
                "# Task 3 — Event Impact Modeling (Rubric-Compliant)\n",
                "\n",
                "**Objective:** Model how events (policies, product launches, infrastructure investments) affect financial inclusion indicators.\n",
                "\n",
                "## Rubric Deliverables\n",
                "1. **Impact Modeling Notebook**: analyze impact links and model event effects on indicators\n",
                "2. **Event–Indicator Association Matrix**: table/heatmap of which events affect which indicators and by how much\n",
                "3. **Historical Validation**: Telebirr launch vs observed mobile money accounts change (4.7% → 9.45%)\n",
                "4. **Methodology Documentation**: assumptions, functional forms, sources, limitations\n",
                "\n",
                "## Production Parity (Hard Requirement)\n",
                "This notebook **must** successfully run `scripts/run_task3.py` to be considered complete.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dcf3adc6",
            "metadata": {},
            "source": [
                "## 0) Setup\n",
                "Run from the repo root (or auto-detect if started inside `notebooks/`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65a55613",
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "\n",
                "import sys\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "PROJECT_ROOT = Path.cwd()\n",
                "if PROJECT_ROOT.name == \"notebooks\":\n",
                "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
                "\n",
                "SRC = PROJECT_ROOT / \"src\"\n",
                "if str(SRC) not in sys.path:\n",
                "    sys.path.insert(0, str(SRC))\n",
                "\n",
                "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\" / \"eda_enriched\"\n",
                "OUTPUTS = PROJECT_ROOT / \"outputs\" / \"task_3\"\n",
                "OUTPUTS.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "assert (PROJECT_ROOT / \"scripts\" / \"run_task3.py\").exists(), \"Missing scripts/run_task3.py\"\n",
                "assert (PROJECT_ROOT / \"src\" / \"fi\" / \"event_effects.py\").exists(), \"Missing src/fi/event_effects.py\"\n",
                "\n",
                "PROJECT_ROOT"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "27f09c23",
            "metadata": {},
            "source": [
                "## 1) Understand the Impact Data (load + join)\n",
                "\n",
                "**Requirement:**\n",
                "- Load impact links\n",
                "- Join with events to get event details\n",
                "- Summarize which events affect which indicators and by how much\n",
                "\n",
                "**Implementation note (your current pipeline):**\n",
                "- The exported `outputs/task_3/impact_links_summary.csv` already includes event metadata (`event_name`, `event_date`, `event_category`, etc.).\n",
                "- Therefore, the join step is *already represented* in the audited summary file.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9023beac",
            "metadata": {},
            "outputs": [],
            "source": [
                "obs_path = DATA_PROCESSED / \"observations.csv\"\n",
                "events_path = DATA_PROCESSED / \"events.csv\"\n",
                "links_summary_path = OUTPUTS / \"impact_links_summary.csv\"\n",
                "\n",
                "obs = pd.read_csv(obs_path)\n",
                "events = pd.read_csv(events_path)\n",
                "links_summary = pd.read_csv(links_summary_path)\n",
                "\n",
                "print(\"obs:\", obs.shape)\n",
                "print(\"events:\", events.shape)\n",
                "print(\"links_summary:\", links_summary.shape)\n",
                "links_summary.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a62e896",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Link summary: which events affect which indicators and by how much\n",
                "view_cols = [\n",
                "    \"link_record_id\",\n",
                "    \"event_record_id\",\n",
                "    \"event_name\",\n",
                "    \"event_category\",\n",
                "    \"event_date\",\n",
                "    \"indicator_code\",\n",
                "    \"impact_direction\",\n",
                "    \"impact_magnitude_pp\",\n",
                "    \"lag_months\",\n",
                "    \"effect_shape\",\n",
                "    \"ramp_years\",\n",
                "    \"evidence_basis\",\n",
                "    \"confidence_link\",\n",
                "    \"impact_magnitude_source\",\n",
                "]\n",
                "view_cols = [c for c in view_cols if c in links_summary.columns]\n",
                "links_summary[view_cols].sort_values([\"event_name\", \"indicator_code\"]).head(30)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39fbc265",
            "metadata": {},
            "source": [
                "## 2) Impact model: effect over time + combining events\n",
                "\n",
                "### Functional form\n",
                "- Each link provides a signed total magnitude (`impact_magnitude_pp`) in percentage points.\n",
                "- Timing is governed by `lag_months` and an effect shape:\n",
                "  - **step**: full effect after lag\n",
                "  - **ramp**: linearly accumulates from 0 → full effect over `ramp_years` (default 3 years)\n",
                "\n",
                "### Month-aware alignment\n",
                "We treat FINDEX snapshot years as observed at **December** of each year. This makes lags and ramps align correctly for events that happen mid-year.\n",
                "\n",
                "### Combining multiple events\n",
                "We assume **additivity in pp space**:\n",
                "- total effect (indicator, year) = sum of all realized effects for that indicator-year across events\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "98a981e6",
            "metadata": {},
            "source": [
                "## 3) Event–Indicator Association Matrix (Rubric)\n",
                "\n",
                "Rows: Events\n",
                "Columns: Indicators\n",
                "Values: **signed total magnitude** (pp)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "03d60f47",
            "metadata": {},
            "outputs": [],
            "source": [
                "assoc_matrix = links_summary.pivot_table(\n",
                "    index=\"event_name\",\n",
                "    columns=\"indicator_code\",\n",
                "    values=\"impact_magnitude_pp\",\n",
                "    aggfunc=\"sum\",\n",
                "    fill_value=0.0,\n",
                ")\n",
                "\n",
                "assoc_matrix.head(20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c207789e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Use your exact path:\n",
                "assoc_path = r\"D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\outputs\\task_3\\event_indicator_association_matrix.csv\"\n",
                "\n",
                "mat_raw = pd.read_csv(assoc_path)\n",
                "\n",
                "meta_cols = [\"event_record_id\", \"event_name\", \"event_category\", \"event_date\"]\n",
                "indicator_cols = [c for c in mat_raw.columns if c not in meta_cols]\n",
                "\n",
                "# index for plotting\n",
                "idx_col = \"event_name\" if \"event_name\" in mat_raw.columns else \"event_record_id\"\n",
                "assoc_matrix = mat_raw.set_index(idx_col)[indicator_cols]\n",
                "\n",
                "# 1) Current/basic heatmap (as-is)\n",
                "plt.figure(\n",
                "    figsize=(min(18, 1.2 * assoc_matrix.shape[1] + 4),\n",
                "             min(12, 0.6 * assoc_matrix.shape[0] + 4))\n",
                ")\n",
                "plt.imshow(assoc_matrix.values, aspect=\"auto\")\n",
                "plt.colorbar(label=\"Estimated effect (pp)\")\n",
                "plt.yticks(range(len(assoc_matrix.index)), assoc_matrix.index, fontsize=8)\n",
                "plt.xticks(range(len(assoc_matrix.columns)), assoc_matrix.columns, rotation=45, ha=\"right\", fontsize=8)\n",
                "plt.title(\"Event–Indicator Association Matrix (pp effects)\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# IMPORTANT: restart kernel after overwriting association_matrix.py (or use autoreload)\n",
                "from fi.association_matrix import plot_heatmap_signed_annotated_inline\n",
                "\n",
                "# 2) Improved signed heatmap: full matrix\n",
                "plot_heatmap_signed_annotated_inline(\n",
                "    assoc_matrix,\n",
                "    title=\"Event–Indicator Association (signed, diverging centered at 0, annotated)\",\n",
                ")\n",
                "plt.show()\n",
                "\n",
                "# 3) Improved signed heatmap: key indicators only\n",
                "key_indicators = [\n",
                "    \"ACC_OWNERSHIP\",\n",
                "    \"ACC_MM_ACCOUNT\",\n",
                "    \"USG_DIGITAL_PAYMENT\",\n",
                "    \"GAP_ACC_OWNERSHIP_MALE_MINUS_FEMALE_PP\",\n",
                "]\n",
                "key_indicators = [c for c in key_indicators if c in assoc_matrix.columns]\n",
                "\n",
                "plot_heatmap_signed_annotated_inline(\n",
                "    assoc_matrix[key_indicators],\n",
                "    title=\"Event–Key Indicators (signed, diverging @0, annotated)\",\n",
                ")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "863e2463",
            "metadata": {},
            "source": [
                "## 4) Build realized effects over time (Lag + Ramp)\n",
                "\n",
                "This uses your current `src/fi/event_effects.py` implementation: `effects_tidy()`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80948229",
            "metadata": {},
            "outputs": [],
            "source": [
                "from fi.event_effects import effects_tidy, sum_effects_over_events, FINDEX_YEAR_GRID\n",
                "\n",
                "event_effects_tidy = effects_tidy(\n",
                "    df_summary=links_summary,\n",
                "    indicators=None,\n",
                "    years=FINDEX_YEAR_GRID,\n",
                "    default_shape=\"ramp\",\n",
                "    default_ramp_years=3.0,\n",
                ")\n",
                "\n",
                "print(\"event_effects_tidy:\", event_effects_tidy.shape)\n",
                "event_effects_tidy.head(20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ea920d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "event_effects_total = sum_effects_over_events(event_effects_tidy)\n",
                "event_effects_total.head(20)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a932543a",
            "metadata": {},
            "source": [
                "## 5) Historical Validation (Telebirr) — Rubric\n",
                "\n",
                "Observed:\n",
                "- Telebirr launched May 2021\n",
                "- Mobile money accounts (ACC_MM_ACCOUNT) went from **4.7% (2021)** to **9.45% (2024)**\n",
                "- Observed delta: **+4.75pp**\n",
                "\n",
                "We compare this with the model-implied Telebirr realized contribution from 2021→2024.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f5ce70c",
            "metadata": {},
            "outputs": [],
            "source": [
                "OBSERVED_DELTA_PP = 9.45 - 4.7\n",
                "\n",
                "tele_effects = event_effects_tidy[\n",
                "    (event_effects_tidy[\"event_name\"].astype(str).str.contains(\"telebirr\", case=False, na=False))\n",
                "    & (event_effects_tidy[\"indicator_code\"] == \"ACC_MM_ACCOUNT\")\n",
                "].copy()\n",
                "\n",
                "tele_pivot = tele_effects.pivot_table(\n",
                "    index=[\"event_record_id\", \"event_name\"],\n",
                "    columns=\"year\",\n",
                "    values=\"effect_pp\",\n",
                "    aggfunc=\"sum\",\n",
                ").fillna(0.0)\n",
                "\n",
                "tele_pivot[\"predicted_delta_2024_minus_2021_pp\"] = tele_pivot.get(2024, 0.0) - tele_pivot.get(2021, 0.0)\n",
                "predicted_delta_pp = float(tele_pivot[\"predicted_delta_2024_minus_2021_pp\"].sum())\n",
                "\n",
                "telebirr_validation = pd.DataFrame([\n",
                "    {\n",
                "        \"observed_delta_pp\": OBSERVED_DELTA_PP,\n",
                "        \"predicted_delta_pp\": predicted_delta_pp,\n",
                "        \"residual_pp\": predicted_delta_pp - OBSERVED_DELTA_PP,\n",
                "    }\n",
                "])\n",
                "\n",
                "tele_pivot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42de3602",
            "metadata": {},
            "outputs": [],
            "source": [
                "telebirr_validation"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd6ecbbc",
            "metadata": {},
            "source": [
                "### If mismatch occurs (required interpretation)\n",
                "\n",
                "If predicted vs observed differ materially:\n",
                "- Adoption curve may be S-shaped rather than linear\n",
                "- Other concurrent events also affected mobile money adoption\n",
                "- Measurement timing/definition changes\n",
                "- Lag/ramp mis-specification\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef8dba19",
            "metadata": {},
            "source": [
                "## 6) Comparable-country evidence + confidence registry (Rubric)\n",
                "\n",
                "For links where Ethiopian pre/post data is insufficient, document comparable-country evidence and mark uncertainty.\n",
                "\n",
                "Here we build an evidence registry directly from the audited summary columns.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d27bd692",
            "metadata": {},
            "outputs": [],
            "source": [
                "evidence_registry = links_summary[[\n",
                "    \"link_record_id\",\n",
                "    \"event_record_id\",\n",
                "    \"event_name\",\n",
                "    \"indicator_code\",\n",
                "    \"impact_magnitude_pp\",\n",
                "    \"lag_months\",\n",
                "    \"effect_shape\",\n",
                "    \"ramp_years\",\n",
                "    \"evidence_basis\",\n",
                "    \"confidence_link\",\n",
                "    \"event_source_name\",\n",
                "    \"event_confidence\",\n",
                "    \"impact_magnitude_source\",\n",
                "]].copy()\n",
                "\n",
                "evidence_registry.head(20)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cd5291e6",
            "metadata": {},
            "source": [
                "## 7) HARD REQUIREMENT: Run canonical pipeline script\n",
                "\n",
                "Notebook fails if `scripts/run_task3.py` fails.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a54c6ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "\n",
                "script = PROJECT_ROOT / \"scripts\" / \"run_task3.py\"\n",
                "proc = subprocess.run(\n",
                "    [sys.executable, str(script)],\n",
                "    cwd=str(PROJECT_ROOT),\n",
                "    capture_output=True,\n",
                "    text=True,\n",
                ")\n",
                "print(proc.stdout)\n",
                "if proc.stderr:\n",
                "    print(\"--- STDERR ---\")\n",
                "    print(proc.stderr)\n",
                "assert proc.returncode == 0, f\"run_task3.py failed with return code {proc.returncode}\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2880fee",
            "metadata": {},
            "source": [
                "## 8) Export Task 3 artifacts\n",
                "\n",
                "Writes rubric outputs to `outputs/task_3/`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cdf2bd12",
            "metadata": {},
            "outputs": [],
            "source": [
                "(OUTPUTS / \"event_indicator_association_matrix.csv\").write_text(assoc_matrix.to_csv(), encoding=\"utf-8\")\n",
                "event_effects_tidy.to_csv(OUTPUTS / \"event_effects_tidy.csv\", index=False)\n",
                "telebirr_validation.to_csv(OUTPUTS / \"telebirr_mm_validation.csv\", index=False)\n",
                "evidence_registry.to_csv(OUTPUTS / \"impact_evidence_registry.csv\", index=False)\n",
                "\n",
                "print(\"wrote:\", OUTPUTS / \"event_indicator_association_matrix.csv\")\n",
                "print(\"wrote:\", OUTPUTS / \"event_effects_tidy.csv\")\n",
                "print(\"wrote:\", OUTPUTS / \"telebirr_mm_validation.csv\")\n",
                "print(\"wrote:\", OUTPUTS / \"impact_evidence_registry.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f19fc52c",
            "metadata": {},
            "source": [
                "## 9) Methodology Documentation (Rubric)\n",
                "\n",
                "### Assumptions\n",
                "- **Additivity**: multiple event effects sum in percentage points\n",
                "- **Lag + (Step/Ramp)**: effects may start after a lag and accumulate linearly over a ramp period\n",
                "- **Snapshot timing**: FINDEX points treated as December of each year for month-aware lag arithmetic\n",
                "\n",
                "### Limitations\n",
                "- Not causal identification (confounding events)\n",
                "- Linear ramp may not match true adoption curves (S-curve)\n",
                "- Sparse survey snapshots limit within-year validation\n",
                "- Comparable-country transfer may not generalize to Ethiopia\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5822796",
            "metadata": {},
            "source": [
                "## Rubric checklist\n",
                "- [x] Impact Modeling Notebook\n",
                "- [x] Event–Indicator Association Matrix\n",
                "- [x] Historical Validation (Telebirr)\n",
                "- [x] Methodology Documentation\n",
                "- [x] Script parity: `scripts/run_task3.py`\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11"
        },
        "title": "Task 3 — Event Impact Modeling (Rubric-Compliant)"
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
