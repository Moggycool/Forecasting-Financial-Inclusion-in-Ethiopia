{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b394f7",
   "metadata": {},
   "source": [
    "# Task 1 — Data Exploration & Schema Validation (Ethiopia Financial Inclusion)\n",
    "\n",
    "## Objective\n",
    "This notebook documents **Task 1** of the pipeline:\n",
    "1. Load the unified dataset (observations + events + impact links + targets)\n",
    "2. Validate the minimum schema and key integrity rules\n",
    "3. Produce EDA artifacts (counts, temporal coverage, indicator coverage)\n",
    "4. Confirm **temporal isolation** so future targets do not leak into historical summaries\n",
    "5. Confirm **referential integrity** for the causal layer (impact links correctly reference events)\n",
    "\n",
    "## Key Deliverables\n",
    "- `counts__record_type.csv`, `counts__pillar.csv` (and optional `counts__category.csv`)\n",
    "- `temporal_range.csv` and subset summaries:\n",
    "  - `temporal_range__observations.csv`\n",
    "  - `temporal_range__events.csv`\n",
    "  - `temporal_range__targets.csv`\n",
    "- `indicator_coverage.csv`\n",
    "- `events.csv`, `impact_links.csv`\n",
    "- diagnostics (only if issues found):\n",
    "  - `invalid_events_with_pillar.csv`\n",
    "  - `invalid_impact_links_missing_parent.csv`\n",
    "  - `invalid_impact_links_unresolved_parent.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e5dc0",
   "metadata": {},
   "source": [
    "## Reproducibility Notes\n",
    "- The project follows a modular structure (`src/fi/...`) and scripts/notebooks should import from `src`.\n",
    "- Outputs are written to a dedicated directory to keep raw vs processed vs diagnostics separated.\n",
    "- We enforce schema and referential integrity **before** any modeling to avoid silent failures later.\n",
    "\n",
    "## Assumptions\n",
    "- `record_type` includes: `observation`, `event`, `impact_link`, `target`\n",
    "- `record_id` is unique per row\n",
    "- `impact_link.parent_id` must reference an existing `event.record_id`\n",
    "- Events should have empty/NA pillar fields (pillar belongs to measured indicators, not events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e510d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook CWD : D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\notebooks\n",
      "Project ROOT : D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Find repo root by walking upward until we see a \"src\" folder\n",
    "CWD = Path.cwd().resolve()\n",
    "ROOT = next((p for p in [CWD, *CWD.parents] if (p / \"src\").is_dir()), None)\n",
    "\n",
    "if ROOT is None:\n",
    "    raise RuntimeError(f\"Could not find project root containing 'src' starting from: {CWD}\")\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"Notebook CWD :\", CWD)\n",
    "print(\"Project ROOT :\", ROOT)\n",
    "\n",
    "from src.fi.io import load_csv, save_csv\n",
    "from src.fi.validation import (\n",
    "    assert_min_schema,\n",
    "    invalid_events_with_pillar,\n",
    "    invalid_impact_links_missing_parent,\n",
    "    invalid_impact_links_unresolved_parent,\n",
    ")\n",
    "from src.fi.explore import (\n",
    "    counts,\n",
    "    temporal_range,\n",
    "    indicator_coverage,\n",
    "    list_events,\n",
    "    list_impact_links,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755addf",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "We load the unified dataset and define an output directory where all Task 1 artifacts will be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd77c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT   : D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\n",
      "IN_PATH: D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\data\\processed\\ethiopia_fi_unified_data__enriched.csv\n",
      "Exists?: True\n",
      "OUT_DIR: D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\data\\processed\\task1_notebook_outputs\n",
      "DIAG_DIR: D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\data\\processed\\task1_notebook_outputs\\diagnostics\n",
      "DIAG_DIR: D:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\data\\processed\\task1_notebook_outputs\\diagnostics\n"
     ]
    }
   ],
   "source": [
    "# Resolve all paths from the repository root (not from notebook CWD)\n",
    "\n",
    "# Guard against running this cell before ROOT is set (or if ROOT ended up as None)\n",
    "if \"ROOT\" not in globals() or ROOT is None:\n",
    "\tCWD = Path.cwd().resolve()\n",
    "\tROOT = next((p for p in [CWD, *CWD.parents] if (p / \"src\").is_dir()), None)\n",
    "\tif ROOT is None:\n",
    "\t\traise RuntimeError(\n",
    "\t\t\tf\"Could not find project root containing 'src' starting from: {CWD}. \"\n",
    "\t\t\t\"Run the setup cell that defines ROOT, or open the notebook inside the repo.\"\n",
    "\t\t)\n",
    "\n",
    "IN_PATH = ROOT / \"data\" / \"processed\" / \"ethiopia_fi_unified_data__enriched.csv\"\n",
    "OUT_DIR = ROOT / \"data\" / \"processed\" / \"task1_notebook_outputs\"\n",
    "DIAG_DIR = OUT_DIR / \"diagnostics\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT   :\", ROOT)\n",
    "print(\"IN_PATH:\", IN_PATH)\n",
    "print(\"Exists?:\", IN_PATH.exists())\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"DIAG_DIR:\", DIAG_DIR)\n",
    "print(\"DIAG_DIR:\", DIAG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162e223",
   "metadata": {},
   "source": [
    "## Step 1 — Load the Unified Dataset\n",
    "The unified dataset contains multiple record types in a single table:\n",
    "- Observations: historical indicator values\n",
    "- Events: discrete changes expected to influence indicators\n",
    "- Impact links: causal mappings from events → indicators (+ magnitude/lag)\n",
    "- Targets: future goal-state anchors (e.g., 2030 targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652b157a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43, 35),\n",
       " ['record_id',\n",
       "  'record_type',\n",
       "  'category',\n",
       "  'pillar',\n",
       "  'indicator',\n",
       "  'indicator_code',\n",
       "  'indicator_direction',\n",
       "  'value_numeric',\n",
       "  'value_text',\n",
       "  'value_type',\n",
       "  'unit',\n",
       "  'observation_date',\n",
       "  'period_start',\n",
       "  'period_end',\n",
       "  'fiscal_year',\n",
       "  'gender',\n",
       "  'location',\n",
       "  'region',\n",
       "  'source_name',\n",
       "  'source_type'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_csv(str(IN_PATH))\n",
    "df.shape, df.columns.tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e1d0dc",
   "metadata": {},
   "source": [
    "## Step 2 — Validate Minimum Schema\n",
    "Before any EDA, we validate the presence of required columns and basic structural assumptions.\n",
    "This prevents downstream errors (e.g., missing `record_type`, missing IDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "841b7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK: minimum schema present'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert_min_schema(df)\n",
    "\"OK: minimum schema present\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60de278",
   "metadata": {},
   "source": [
    "## Step 3 — Dataset Composition\n",
    "We examine how many rows belong to each record type and pillar.\n",
    "This is used to confirm expected proportions (e.g., many observations, fewer events/targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5835dc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   record_type   n\n",
       " 0  observation  30\n",
       " 1        event  10\n",
       " 2       target   3,\n",
       "           pillar   n\n",
       " 0         ACCESS  16\n",
       " 1          USAGE  11\n",
       " 2        (empty)  10\n",
       " 3         GENDER   5\n",
       " 4  AFFORDABILITY   1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_record_type = counts(df, \"record_type\")\n",
    "ct_pillar = counts(df, \"pillar\")\n",
    "\n",
    "save_csv(ct_record_type, str(OUT_DIR / \"counts__record_type.csv\"))\n",
    "save_csv(ct_pillar, str(OUT_DIR / \"counts__pillar.csv\"))\n",
    "\n",
    "ct_record_type, ct_pillar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3ea31",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'event_end_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'event_end_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNo category column found\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# print(temporal_range(df, \"event_start_date\"))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtemporal_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mevent_end_date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\src\\fi\\explore.py:22\u001b[39m, in \u001b[36mtemporal_range\u001b[39m\u001b[34m(df, date_col)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtemporal_range\u001b[39m(df: pd.DataFrame, date_col: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mobservation_date\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get temporal range information for a given date column.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     s = pd.to_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m]\u001b[49m, errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmin_date\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m s.min() \u001b[38;5;129;01mis\u001b[39;00m pd.NaT \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(s.min().date()),\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_date\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m s.max() \u001b[38;5;129;01mis\u001b[39;00m pd.NaT \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(s.max().date()),\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mn_parsed\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(s.notna().sum()),\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mn_total\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s)),\n\u001b[32m     28\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Week10\\Forecasting-Financial-Inclusion-in-Ethiopia\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'event_end_date'"
     ]
    }
   ],
   "source": [
    "if \"category\" in df.columns:\n",
    "    ct_category = counts(df, \"category\")\n",
    "    save_csv(ct_category, str(OUT_DIR / \"counts__category.csv\"))\n",
    "    ct_category.head(30)\n",
    "else:\n",
    "    \"No category column found\"\n",
    "cov = indicator_coverage(df)\n",
    "save_csv(cov, str(OUT_DIR / \"indicator_coverage.csv\"))\n",
    "cov.head(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247602e",
   "metadata": {},
   "source": [
    "## Step 4 — Temporal Coverage (Overall)\n",
    "We compute the minimum and maximum dates across the whole dataset.\n",
    "\n",
    "⚠️ Note: The unified dataset includes **future targets**, which can cause temporal summaries to appear\n",
    "to extend beyond observed history. Therefore, we also compute **record_type-specific** temporal ranges\n",
    "to prevent temporal leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f27df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_date': '2014-12-31',\n",
       " 'max_date': '2030-12-31',\n",
       " 'n_parsed': 43,\n",
       " 'n_total': 43}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_all = temporal_range(df, \"observation_date\")\n",
    "pd.DataFrame([tr_all]).to_csv(OUT_DIR / \"temporal_range.csv\", index=False)\n",
    "tr_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6120f",
   "metadata": {},
   "source": [
    "## Step 4b — Temporal Isolation by Record Type\n",
    "We compute temporal ranges separately for:\n",
    "- Observations (historical / measured)\n",
    "- Events (policy/product changes)\n",
    "- Targets (future goal anchors)\n",
    "\n",
    "This ensures **future targets** do not get mistaken for historical observations during EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469b77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'min_date': '2014-12-31',\n",
       "  'max_date': '2025-12-31',\n",
       "  'n_parsed': 30,\n",
       "  'n_total': 30},\n",
       " {'min_date': '2021-05-17',\n",
       "  'max_date': '2025-12-18',\n",
       "  'n_parsed': 10,\n",
       "  'n_total': 10},\n",
       " {'min_date': '2025-12-31',\n",
       "  'max_date': '2030-12-31',\n",
       "  'n_parsed': 3,\n",
       "  'n_total': 3})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subset(df: pd.DataFrame, record_type: str) -> pd.DataFrame:\n",
    "    rt = df[\"record_type\"].fillna(\"\").astype(str).str.strip().str.lower()\n",
    "    return df.loc[rt.eq(record_type)].copy()\n",
    "\n",
    "df_obs = subset(df, \"observation\")\n",
    "df_evt = subset(df, \"event\")\n",
    "df_tgt = subset(df, \"target\")\n",
    "\n",
    "def save_temporal(name: str, dfx: pd.DataFrame):\n",
    "    tr = temporal_range(dfx, \"observation_date\") if len(dfx) else {\"min_date\": None, \"max_date\": None, \"n_non_null_dates\": 0}\n",
    "    pd.DataFrame([tr]).to_csv(OUT_DIR / f\"{name}.csv\", index=False)\n",
    "    return tr\n",
    "\n",
    "tr_obs = save_temporal(\"temporal_range__observations\", df_obs)\n",
    "tr_evt = save_temporal(\"temporal_range__events\", df_evt)\n",
    "tr_tgt = save_temporal(\"temporal_range__targets\", df_tgt)\n",
    "\n",
    "tr_obs, tr_evt, tr_tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683be31e",
   "metadata": {},
   "source": [
    "## Step 5 — Indicator Coverage (Observations Only)\n",
    "We summarize the number of observed data points per indicator and the date span for each.\n",
    "This helps identify:\n",
    "- sparse indicators\n",
    "- indicators with gaps\n",
    "- indicators with only anchor points\n",
    "\n",
    "This table is a key input to Task 2 (S-curve fitting and baseline trend modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed4464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = indicator_coverage(df)\n",
    "save_csv(cov, str(OUT_DIR / \"indicator_coverage.csv\"))\n",
    "cov.head(30)\n",
    "cov.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47833551",
   "metadata": {},
   "source": [
    "## Step 6 — Events and Impact Links\n",
    "We extract and save:\n",
    "- the event table (event_id, names/dates/metadata)\n",
    "- the impact_link table (which event affects which indicator, with parameters such as lag/magnitude)\n",
    "\n",
    "These tables are used in Task 3 / causal impact logic and should be validated carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6853f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   record_id        category                               indicator  \\\n",
       " 33  EVT_0001  product_launch                         Telebirr Launch   \n",
       " 41  EVT_0009          policy                 NFIS-II Strategy Launch   \n",
       " 34  EVT_0002    market_entry    Safaricom Ethiopia Commercial Launch   \n",
       " 35  EVT_0003  product_launch                  M-Pesa Ethiopia Launch   \n",
       " 36  EVT_0004  infrastructure        Fayda Digital ID Program Rollout   \n",
       " 37  EVT_0005          policy         Foreign Exchange Liberalization   \n",
       " 38  EVT_0006       milestone     P2P Transaction Count Surpasses ATM   \n",
       " 39  EVT_0007     partnership            M-Pesa EthSwitch Integration   \n",
       " 42  EVT_0010         pricing       Safaricom Ethiopia Price Increase   \n",
       " 40  EVT_0008  infrastructure  EthioPay Instant Payment System Launch   \n",
       " \n",
       "        indicator_code observation_date    source_name confidence  \n",
       " 33       EVT_TELEBIRR       2021-05-17  Ethio Telecom       high  \n",
       " 41          EVT_NFIS2       2021-09-01            NBE       high  \n",
       " 34      EVT_SAFARICOM       2022-08-01           News       high  \n",
       " 35          EVT_MPESA       2023-08-01      Safaricom       high  \n",
       " 36          EVT_FAYDA       2024-01-01           NIDP       high  \n",
       " 37      EVT_FX_REFORM       2024-07-29            NBE       high  \n",
       " 38      EVT_CROSSOVER       2024-10-01      EthSwitch       high  \n",
       " 39  EVT_MPESA_INTEROP       2025-10-27      EthSwitch       high  \n",
       " 42   EVT_SAFCOM_PRICE       2025-12-15           News       high  \n",
       " 40       EVT_ETHIOPAY       2025-12-18  NBE/EthSwitch       high  ,\n",
       " Empty DataFrame\n",
       " Columns: [record_id, parent_id, pillar, related_indicator, indicator_code, impact_direction, impact_magnitude, lag_months, evidence_basis, confidence]\n",
       " Index: [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = list_events(df)\n",
    "impact_links = list_impact_links(df)\n",
    "\n",
    "save_csv(events, str(OUT_DIR / \"events.csv\"))\n",
    "save_csv(impact_links, str(OUT_DIR / \"impact_links.csv\"))\n",
    "\n",
    "events.head(20), impact_links.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c217608",
   "metadata": {},
   "source": [
    "## Step 7 — Referential Integrity (Causal Layer Readiness)\n",
    "\n",
    "We run three checks:\n",
    "1) **Events must not carry indicator pillar fields**  \n",
    "   Events are exogenous drivers; pillars belong to measured indicators.\n",
    "2) **Impact links must have a `parent_id`**  \n",
    "   Every impact link row must identify which event it references.\n",
    "3) **Impact links must resolve to an existing event record**  \n",
    "   Every `impact_link.parent_id` must match an `event.record_id`.\n",
    "Any failures are written to `diagnostics/` for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_events = invalid_events_with_pillar(df)\n",
    "bad_missing_parent = invalid_impact_links_missing_parent(df)\n",
    "bad_unresolved_parent = invalid_impact_links_unresolved_parent(df)\n",
    "\n",
    "len(bad_events), len(bad_missing_parent), len(bad_unresolved_parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ba331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Diagnostics written (only for non-empty failures).'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(bad_events) > 0:\n",
    "    save_csv(bad_events, str(DIAG_DIR / \"invalid_events_with_pillar.csv\"))\n",
    "\n",
    "if len(bad_missing_parent) > 0:\n",
    "    save_csv(bad_missing_parent, str(DIAG_DIR / \"invalid_impact_links_missing_parent.csv\"))\n",
    "\n",
    "if len(bad_unresolved_parent) > 0:\n",
    "    save_csv(bad_unresolved_parent, str(DIAG_DIR / \"invalid_impact_links_unresolved_parent.csv\"))\n",
    "\n",
    "\"Diagnostics written (only for non-empty failures).\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac939b",
   "metadata": {},
   "source": [
    "## Step 8 — Interpretation Summary (Report-Ready Notes)\n",
    "\n",
    "Use the following bullets directly in the report:\n",
    "\n",
    "- **Dataset size & composition:** confirm the number of observations/events/targets.\n",
    "- **Temporal coverage:** report observation-only range (prevents target leakage).\n",
    "- **Indicator coverage:** identify which indicators have sufficient density for trend fitting vs only anchors.\n",
    "- **Causal layer readiness:** confirm zero unresolved parent IDs and no invalid event pillar fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b52c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_records': 43, 'record_type_counts': {'observation': 30, 'event': 10, 'target': 3}, 'overall_min_date': '2014-12-31', 'overall_max_date': '2030-12-31', 'obs_min_date': '2014-12-31', 'obs_max_date': '2025-12-31', 'tgt_min_date': '2025-12-31', 'tgt_max_date': '2030-12-31', 'evt_min_date': '2021-05-17', 'evt_max_date': '2025-12-18', 'invalid_events_with_pillar': 0, 'impact_links_missing_parent_id': 0, 'impact_links_unresolved_parent_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((43, 35),\n",
       " ['record_id',\n",
       "  'record_type',\n",
       "  'category',\n",
       "  'pillar',\n",
       "  'indicator',\n",
       "  'indicator_code',\n",
       "  'indicator_direction',\n",
       "  'value_numeric',\n",
       "  'value_text',\n",
       "  'value_type',\n",
       "  'unit',\n",
       "  'observation_date',\n",
       "  'period_start',\n",
       "  'period_end',\n",
       "  'fiscal_year',\n",
       "  'gender',\n",
       "  'location',\n",
       "  'region',\n",
       "  'source_name',\n",
       "  'source_type'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt_counts = dict(zip(ct_record_type[\"record_type\"], ct_record_type[\"n\"]))\n",
    "\n",
    "summary = {\n",
    "    \"total_records\": len(df),\n",
    "    \"record_type_counts\": rt_counts,\n",
    "    \"overall_min_date\": tr_all.get(\"min_date\"),\n",
    "    \"overall_max_date\": tr_all.get(\"max_date\"),\n",
    "    \"obs_min_date\": tr_obs.get(\"min_date\"),\n",
    "    \"obs_max_date\": tr_obs.get(\"max_date\"),\n",
    "    \"tgt_min_date\": tr_tgt.get(\"min_date\"),\n",
    "    \"tgt_max_date\": tr_tgt.get(\"max_date\"),\n",
    "    \"evt_min_date\": tr_evt.get(\"min_date\"),\n",
    "    \"evt_max_date\": tr_evt.get(\"max_date\"),\n",
    "    \"invalid_events_with_pillar\": len(bad_events),\n",
    "    \"impact_links_missing_parent_id\": len(bad_missing_parent),\n",
    "    \"impact_links_unresolved_parent_id\": len(bad_unresolved_parent),\n",
    "}\n",
    "\n",
    "save_csv(pd.DataFrame([summary]), str(OUT_DIR / \"data_summary.csv\"))\n",
    "print(summary)\n",
    "df.shape, df.columns.tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf2de4",
   "metadata": {},
   "source": [
    "## Outputs Checklist (what should exist on disk)\n",
    "\n",
    "In `data/processed/task1_notebook_outputs/`:\n",
    "- `counts__record_type.csv`\n",
    "- `counts__pillar.csv`\n",
    "- (optional) `counts__category.csv`\n",
    "- `temporal_range.csv`\n",
    "- `temporal_range__observations.csv`\n",
    "- `temporal_range__events.csv`\n",
    "- `temporal_range__targets.csv`\n",
    "- `indicator_coverage.csv`\n",
    "- `events.csv`\n",
    "- `impact_links.csv`\n",
    "\n",
    "In `data/processed/task1_notebook_outputs/diagnostics/` (only if errors detected):\n",
    "- `invalid_events_with_pillar.csv`\n",
    "- `invalid_impact_links_missing_parent.csv`\n",
    "- `invalid_impact_links_unresolved_parent.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69089960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
